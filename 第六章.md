第7章 贝叶斯分类器
==============
#### 7.1 贝叶斯决策论
贝叶斯决策论是概率框架下实施决策的基本方
法.对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑
如何基于这些概率和误判损失来选择最优的类别标记.下面我们以多分类任务
为例来解释其基本原理.   
假设有N种可能的类别标记，即;Y = {c1,c2,,……，cN}, lngtij是将一个真实
标记为cj的样本误分类为ci所产生的损失.基于后验概率P(ci|x)可获得将
样本x分类为ci所产生的期望损失,即在样本x上的“条件风
险"  
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20090626.png)  
我们的任务是寻找一个判定准则h:X->Y以最小化总体风险  
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20090707.png)  
显然，对每个样本若九能最小化条件风险R(h(x) | x),则总体风险R(h)也
将被最小化.这就产生了贝叶斯判定准则:为最小化总体
风险，只需在每个样本上选择那个能使条件风险R(c|x)时最小的类别标记，即  
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20091842.png)  
此时，h* 称为贝叶斯最优分类器,与之对应的总体风
险R(h*)称为贝叶斯风险(Bayes risk).1-R(h*)反映了分类器所能达到的最
好性能，即通过机器学习所能产生的模型精度的理论上限.  
具体来说,若目标是最小化分类错误率，则误判损失lngtij可写为   
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20092703.png)  
此时条件风险   
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20092922.png)  
于是，最小化分类错误率的贝叶斯最优分类器为  
![imaeg](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20092938.png)  
即对每个样本与选择能使后验概率P(c|x)时最大的类别标记.  
不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概
率P(c | x).然而，在现实任务中这通常难以直接获得.从这个角度来看，机
器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率
P{c | X).大体来说，主要有两种策略：给定皿可通过直接建模P(c|x)来
预测c,这样得到的是"判别式模型”;也可先对联合
概率分布F(©,c)建模，然后再由此获得P(c | x),这样得到的是“生成式模
型” (generative models).显然，前面介绍的决策树、BP神经网络、支持向量
机等，都可归入判别式模型的范畴.对生成式模型来说，必然考虑  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20095845.png)  
其中，P(c)是类“先验”(prior)概率；P(x | c)是样本。相对于类标记c的类
条件概率,或称为“似然” ; P(x)是
用于归一化的“证据”(evidence)因子.对给定样本吃证据因子户(①)与类标
记无关，因此估计P(c|x)的问题就转化为如何基于训练数据D来估计先验
P(c)和似然 P{x | c).
#### 7.2 极大似然估计
估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形
式，再基于训练样本对概率分布的参数进行估计.具体地，记关于类别c的类条
件概率为p(x|c),假设P(x|c)具有确定的形式并且被参数向量thetac唯一确定,
则我们的任务就是利用训练集D估计参数thetac.为明确起见，我们将P(x|c)记
为 P(x|thetac).  
贝叶斯学派则认为参数是未观察
到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后
基于观测到的数据来计算参数的后验分布.  频率主义学
派认为参数虽然未知，但却是客观存在的固定值，因此，可通过优
化似然函数等准则来确定参数值；  
令Dc表示训练集D中第c类样本组成的集合，假设这些样本是独立同分
布的，则参数0c对于数据集Dc的似然是  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20095306.png)  
对ec进行极大似然估计，就是去寻找能最大化似然p(Dc|ec)的参数值此.直
观上看，极大似然估计是试图在。c所有可能的取值中，找到一个能使数据出现
的“可能性”最大的值.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20100054.png)
#### 7.3 朴素贝叶斯分类器 
类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接
估计而得.为避开这个障碍，朴素贝叶斯分类器用了
"属性条件独立性假设”:对
已知类别，假设所有属性相互独立.换言之，假设每个属性独立地对分类结果发
生影响.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20100202.png)  
在现实任务中朴素贝叶斯分类器有多种使用方式.例如，若任务对预测速
度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事
先计算好存储起来，这样在进行预测时只需“查表”即可进行判别；若任务数
据更替频繁，则可采用“懒惰学习”(lazy learning)方式，先不进行任何训练,
待收到预测请求时再根据当前数据集进行概率估值；若数据不断增加，则可在
现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正即可
实现增量学习.
#### 7.4 半朴素贝叶斯分类器
为了降低贝叶斯公式(7.8)中估计后验概率P(c|x)时的困难，朴素贝叶斯分
类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立.于
是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称
为“半朴素贝叶斯分类器”的学习方法.  
半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信
息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依
赖关系."独依赖估计” 是半朴素贝叶
斯分类器最常用的一种策略.顾名思议，所谓“独依赖”就是假设每个属性在
类别之外最多仅依赖于一个其他属性，即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20101308.png)  
最直接的做法是假设所有属性都依赖于同一个属性，称为“超父”, 然后通过交叉验证等模型选择方法来确定超父属性，由此形成了
SPODE方法  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20101433.png)  
TAN (Tree Augmented naive Bayes)则是在最大带
权生成树算法的基
础上，通过以下步骤将属性间依赖关系约简为如上图所示的树形结构：   
⑴计算任意两个属性之间的条件互信息(conditional mutual information)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20101826.png)  
(2) 以属性为结点构建完全图，任意两个结点之间边的权重设为
I(xi,xj,y)；  
(3) 构建此完全图的最大带权生成树，挑选根变量，将边置为有向；  
(4)加入类别结点y.增加从y到每个属性的有向边.  
AODE (Averaged One-Dependent Estimator)是一种
集成学习参见第8章. 基于集成学习机制、更为强大的独依赖分类器.与SPODE通过模型选择确定
超父属性不同，AODE尝试将每个属性作为超父来构建SPODE,然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果，即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20102122.png)  
不难看出，与朴素贝叶斯分类器类似,AODE的训练过程也是“计数”，即
在训练数据集上对符合条件的样本进行计数的过程.与朴素贝叶斯分类器相
似,AODE无需模型选择，既能通过预计算节省预测时间，也能采取懒惰学习方
式在预测时再进行计数，并且易于实现增量学习.
#### 7.5 贝叶斯网 
贝叶斯网(Bayesian network)亦称“信念网"(belief network),它借助有向
无环图(Directed Acyclic Graph,简称DAG)来刻画属性之间的依赖关系，并使用条件概率表(Conditional Probability Table,简称CPT)来描述属性的联合概
率分布.
#### 7.5.1 结构
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20103142.png)  
在“同父” (common parent)结构中，给定父结点x1的取值，则x3与
x4条件独立.在“顺序”结构中，给定x的值，则y与z条件独立.V型结构(Vstructure)亦称“冲撞”结构，给定子结点的取值，与X2必不独立；奇妙
的是，若x4的取值完全未知，则V型结构下x1与x2却是相互独立的.我们做
一个简单的验证：  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20103040.png)  
为了分析有向图中变量间的条件独立性，可使用“有向分离”(Dseparation).我们先把有向图转变为一个无向图：  
•找出有向图中的所有V型结构，在V型结构的两个父结点之间加上一条无向边；  
•将所有有向边改为无向边.  
#### 7.5.2 学习
若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对
简单，只需通过对训练样本“计数”,估计出每个结点的条件概率表即可.但在
现实应用中我们往往并不知晓网络结构，于是，贝叶斯网学习的首要任务就是
根据训练数据集来找出结构最“恰当”的贝叶斯网.“评分搜索”是求解这一
问题的常用办法.具体来说，我们先定义一个评分函数(score function),以此来
评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优
的贝叶斯网.显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归
纳偏好.  

常用评分函数通常基于信息论准则，此类准则将学习问题看作一个数据压
缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时
编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需
的字节长度.对贝叶斯网学习而言，模型就是一个贝叶斯网，同时，每个贝叶斯
网描述了一个在训练数据上的概率分布，自有一套编码机制能使那些经常出
现的样本有更短的编码.于是，我们应选择那个综合编码长度(包括描述网络
和编码数据)最短的贝叶斯网，这就是"最小描述长度”(Minimal Description
Length,简称MDL)准则.
#### 7.5.3 推断
贝叶斯网训练好之后就能用来回答“查询” (query),即通过一些属性变量
的观测值来推测其他属性变量的取值.例如在西瓜问题中，若我们观测到西瓜
色泽青绿、敲声浊响、根蒂蜷缩，想知道它是否成熟、甜度如何•这样通过已
知变量观测值来推测待查询变量的过程称为“推断” (inference),已知变量观
测值称为“证据"(evidence).   
令Q = {Q1,Q2,..,Q3}表示待查询变量, E = ｛E1,E2,...,Ek｝为证据变
量，已知其取值为e ={e1,e2,……,ek}.目标是计算后验概率P(Q = q | E = e),
其中q =｛q1,q2,……,qn｝是待查询变量的一组取值.  
吉布斯采样算法先随机产生一个与证据E = e 一致的样本
q°作为初始点，然后每步从当前样本出发产生下一个样本.具体来说，在第t
次采样中，算法先假设q^t=q^t-1,然后对非证据变量逐个进行采样改变其取值,
采样概率根据贝叶斯网B和其他变量的当前取值(即Z = z)计算获得.假定经
过T次采样得到的与q一致的样本共有nq个，则可近似估算出后验概率  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20104810.png)  
实质上，吉布斯采样是在贝叶斯网所有变量的联合状态空间与证据E = e
一致的子空间中进行“随机漫步"(random walk).每一步仅依赖于前一步
的状态，这是一个"马尔可夫链” (Markov chain).在一定条件下，无论从
什么初始状态开始，马尔可夫链第t步的状态分布在t -> oo时必收敛于一
个平稳分布(stationary distribution);对于吉布斯采样来说，这个分布恰好是
P(Q | E = e).因此，在T很大时，吉布斯采样相当于根据P(Q | E = e)采样,
从而保证了式(7.33)收敛于P(Q = q | E = e).  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20104913.png)  
#### 7.6 EM算法
未观测变量的学名是“隐变量”(latent variable).令X表示已观测变量
集，Z表示隐变量集，0表示模型参数.若欲对0做极大似然估计，则应最大化
对数似然   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20105547.png)  
然而由于Z是隐变量，上式无法直接求解.此时我们可通过对Z计算期望，来最大化巳观测数据的对数"边际似然” (marginal likelihood)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20105618.png)  
EM (Expectation-Maximization)算法［Dempster et al., 1977］是常用的估
计参数隐变量的利器，它是一种迭代式的方法，其基本想法是：若参数Q已知,
则可根据训练数据推断出最优隐变量Z的值(E步);反之，若Z的值已知，则可
方便地对参数0做极大似然估计(M步).  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20105916.png)  
