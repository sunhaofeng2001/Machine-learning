第5章 神经网络
=====
#### 5.1 神经元模型
“神经网络是由具有适应性的
简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实
世界物体所作出的交互反应”.我们在机器学习中谈论神经网
络时指的是“神经网络学习”，或者说，是机器学习与神经网络这两个学科领
域的交叉部分.  
神经网络中最基本的成分是神经元(neuron)模型，即上述定义中的“简单
单元”.在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时,
就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神
经元的电位超过了一个“阈值” (threshold),那么它就会被激活，即“兴奋”
起来，向其他神经元发送化学物质.  
“M-P神经元模型”.在这个模型中，神经元接
收到来自仅个其他神经元传递过来的输入信号，这些输入信号通过带权重的连
接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比
较，然后通过“激活函数”(activation function)处理以产生神经元的输出.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20195041.png)  
理想中的激活函数是图5.2(a)所示的阶跃函数，它将输入值映射为输出
值“0”或“1” ,显然“1”对应于神经元兴奋，“0”对应于神经元抑制.然
而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用Sigmoid
函数作为激活函数.典型的Sigmoid函数如图5.2(b)所示，它把可能在较大
范围内变化的输入值挤压到(0,1)输出值范围内，因此有时也称为“挤压函
数"(squashing function).  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20195216.png)  
把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络.
#### 5.2 感知机与多层网络
感知机(Perceptron)由两层神经元组成，如图5.3所示，输入层接收外
界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单
元” (threshold logic unit).  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20195524.png)  
.感知机学习规则非常简单，对训练样例(x,y),若当前感知机的输出为们则感知机权
重将这样调整:  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20200828.png)  
需注意的是，感知机只有输出层神经元进行激活函数处理，即只拥有一层
功能神经元(functional neuron),其学习能力非常有限.事实上，上述与、或、 非问题都是线性可分的问题.可以证明若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如图感知机的学习过程一定会收M(converge)而求得适当的权向
量m = (w1;W2;……;wn+i)；否则感知机学习过程将会发生振荡(fluctuation), w
难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图5.4(d)所示的
异或这样简单的非线性可分问题.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20201243.png)  
要解决非线性可分问题，需考虑使用多层功能神经元.简单的两层感知机就能解决异或问题.输出层与输入层之间的一
层神经元，被称为隐层或隐含层(hidden layer),隐含层和输出层神经元都是拥
有激活函数的功能神经元.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20201605.png)  
更一般的，常见的神经网络是形如图所示的层级结构，每层神经元与下
一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接.这样的
神经网络结构通常称为“多层前馈神经网络".,其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行
加工，最终结果由输出层神经元输出；换言之，输入层神经元仅是接受输入，不
进行函数处理，隐层与输出层包含功能神经元.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20201618.png)  
#### 5.3 误差逆传播算法
多层网络的学习能力比单层感知机强得多.欲训练多层网络，式(5.1)的
简单感知机学习规则显然不够了，需要更强大的学习算法.误差逆传播(euor
BackPropagation,简称BP)算法就是其中最杰出的代表，它是迄今最成功的神
经网络学习算法.现实任务中使用神经网络时，大多是在使用BP算法进行训
练.  
符号:  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20203143.png)    
推导:  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20203637.png)   
#### 5.4 全局最小与局部极小
若用E表示神经网络在训练集上的误差，则它显然是关于连接权w和阈
值。的函数.此时，神经网络的训练过程可看作一个参数寻优过程，即在参数空
间中，寻找一组最优参数使得E最小.
#### 5.5 其他常见神经网络
#### 5.5.1 RBF网络
RBF网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函
数，而输出层则是对隐层神经元输出的线性组合.假定输入为d维向量力，输出
为实值，则RBF网络可表示为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20205131.png)  
其中q为隐层神经元个数,ci和wi分别是第i个隐层神经元所对应的中心和权
重，p{xi,ci)是径向基函数，这是某种沿径向对称的标量函数，通常定义为样本
x到数据中心ci之间欧氏距离的单调函数.常用的高斯径向基函数形如  
![iamge](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20205235.png)  
通常采用两步过程来训练RBF网络：第一步，确定神经元中心ci,常用的
方式包括随机采样、聚类等;第二步，利用BP算法等来确定参数wi和Bi.
#### 5.5.2 ART网络
竞争型学习是神经网络中一种常用的无监督学习
策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞
争获胜的神经元被激活，其他神经元的状态被抑制.这种机制亦称“胜者通
吃"原则.  
ART网络是竞争型学习的重要代表.该网络由比较层、识别层、识别
阈值和重置模块构成.其中「比较层负责接收输入样本，并将其传递给识别层神
经元.识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增
长以增加新的模式类.  
在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神
经元.竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式
类的代表向量之间的距离，距离最小者胜.获胜神经元将向其他识别层神经元
发送信号，抑制其激活.若输入向量与获胜神经元所对应的代表向量之间的相
似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络
连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的
相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值，则重
置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量.
显然，识别阈值对ART网络的性能有重要影响.当识别阈值较高时,输入样
本将会被分成比较多、比较精细的模式类，而如果识别阈值较低，则会产生比
较少、比较粗略的模式类.
ART比较好地缓解了竞争型学习中的“可塑性-稳定性窘境” ,可塑性是指神经网络要有学习新知识的能力，而稳定性则
是指神经网络在学习新知识时要保持对旧知识的记忆.这就使得ART网络具有
一个很重要的优点：可进行增量学习或在线学习.  
#### 5.5.3 SOM 网络
SOM(Self-Organizing Map,自组织映射)网络［Kohonen, 1982］是一种竞
争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二
维)，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点
映射到网络输出层中的邻近神经元.  
如图所示，SOM网络中的输出层神经元以矩阵方式排列在二维空间
中，每个神经元都拥有一权向量，网络在接收输入向量后，将会确定输出层获
胜神经元，它决定了该输入向量在低维空间中的位置.SOM的训练目标就是为
每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的.  
SOM的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会
计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜
者,称为最佳匹配单元(best matching unit).然后，最佳匹配单元及其邻近神经
元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小.这个过
程不断迭代，直至收敛.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20210102.png)  
#### 5.5.4 级联相关网络
一般的神经网络模型通常假定网络结构是事先固定的，训练的目的是利用
练样本来确定合适的连接权、阈值等参数.与此不同，结构自适应网络则将
网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点
的网络结构.级联相关网络是结构自适应网络的重要代表.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20210625.png)  
级联相关网络有两个主要成分：“级联”和“相关”.级联是指建立层次
连接的层级结构.在开始训练时，网络只有输入层和输出层，处于最小拓扑结
构；随着训练的进行，如图5.12所示，新的隐层神经元逐渐加入，从而创建起层
级结构.当新的隐层神经元加入时，其输入端连接权值是冻结固定的.相关是
指通过最大化新神经元的输出与网络误差之间的相关性来训练相
关的参数.  
与一般的前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经
元数目，且训练速度较快，但其在数据较小时易陷入过拟合.  
#### 5.5.5 Elman网络
与前馈神经网络不同，“递归神经网络"允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号.这
样的结构与信息反馈过程，使得网络在t时刻的输出状态不仅与/时刻的输入
有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化.
Elman网络是最常用的递归神经网络之一，其结构如图
所示，它的结构与多层前馈网络很相似，但隐层神经元的输出被反馈回来，与下
一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入.隐
层神经元通常采用Sigmoid激活函数，而网络的训练则常通过推广的BP算法
进行.  
![image]()  
#### 5.5.6 Boltzmann机
神经网络中有一类模型是为网络状态定义一个“能量” ,能量. 最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数.
Boltzmann 机就是一种"基于能量的模型”
,常见结构如图5.14(a)所示，其神经元分为两层：显层与隐层.显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达.Boltzmann机中
的神经元都是布尔型的，即只能取0、1两种状态，状态1表示激活，状态0表
示抑制.令向量s属于{0, l}^n表示n个神经元的状态,Wij表示神经元i与j之间
的连接权，th表示神经元i的阈值，则状态向量s所对应的Boltzmann机能量
定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20211442.png)  
若网络中的神经元以任意不依赖于输入值的顺序进行更新，则网络最终将达到Boltzmann分布，此时状态向量s出现的概率将仅由其能量与所有可能状态向量的能量确定：   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20211619.png)
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20211629.png)  
Boltzmann机的训练过程就是将每个训练样本视为一个状态向量，使
其出现的概率尽可能大.标准的Boltzmann机是一个全连接图，训练网络的
复杂度很高，这使其难以用于解决现实任务.现实中常采用受限Boltzmann
机.如图所示，受限 Boltzmann 机仅保留显层与隐层之间的连接，从而将Boltzmann机结构由完全图简
化为二部图.    
受限 Boltzmann 机常用 “对比散度” (Contrastive Divergence,简称
CD)算法［Hinton, 2010］来进行训练.假定网络中有d个显层神经元和q
个隐层神经元，令。和九分别表示显层与隐层的状态向量，则由于同一层内不
存在连接，有  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20211816.png)  
CD算法对每个训练样本v,先根据式(5.23)计算出隐层神经元状态的概率分布,
然后根据这个概率分布采样得到h.此后，类似地根据式(5.22)从h产生村,再
从丁产生阳；连接权的更新公式为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20211825.png)  
#### 5.6 深度学习
理论上来说，参数越多的模型复杂度越高、“容量”(capacity)越大，这意
味着它能完成更复杂的学习任务.但一般情形下，复杂模型的训练效率低，易陷
入过拟合.  
典型的深度学习模型就是很深层的神经网络.显然，对神经网络模型，提高
容量的一个简单办法是增加隐层的数目.隐层多了，相应的神经元连接权、阈
值等参数就会更多.模型复杂度也可通过单纯增加隐层神经元的数目来实现,
前面我们谈到过，单隐层的多层前馈网络已具有很强大的学习能力；但从增加
模型复杂度的角度来看，增加隐层的数目显然比增加隐层神经元的数目更有效,
因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数嵌
套的层数哗然而，多隐层神经网络难以直接用经典算法(例如标准BP算法)进行
训练，因为误差在多隐层内逆传播时，往往会“发散” 而不能收敛到
稳定状态.  
无监督逐层训练是多隐层网络训练的
有效手段，其基本思想是每次训练一层隐结点，训练时将上一层隐结点的输
出作为输入,而本层隐结点的输出作为下一层隐结点的输入，这称为“预训
练”;在预训练全部完成后，再对整个网络进行“微调”训练.
例如，在深度信念网络中，每层都是一个受限Boltzmann 即整个网络可视为若干个
RBM堆叠而得.在使用无监督逐层训练时，首先训练第一层，这是关于训练样
本的RBM模型，可按标准的RBM训练;然后，将第一层预训练好的隐结点视为
第二层的输入结点，对第二层进行预训练;……各层预训练完成后，再利用BP
算法等对整个网络进行训练.
事实上，“预训练+微调”的做法可视为将大量参数分组，对每组先找到局
部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优.
这样就在利用了模型大量参数所提供的自由度的同时，有效地节省了训练开销.
另一种节省训练开销的策略是"权共享” ,即让一组
神经元使用相同的连接权.这个策略在卷积神经网络中发挥了
重要作用.
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20213102.png)  
