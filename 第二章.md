第3章 线性模型
=====
#### 3.1 基本形式
给定由d个属性描述的示例x =(x1;x2;x3……xd)，其中xi是x在第i个属
性上的取值，线性模型(linear model)试图学得一个通过属性的线性组合来进行
预测的函数，即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20231201.png)  
一般用向量形式写成  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20231211.png)  
其中w =(W1； w2;...; Wd). s和b学得之后，模型就得以确定.
#### 3.2 线性回归
输入属性的数目只有一个.线性回归试图学得  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232046.png)  
让均方误差最小化，即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232216.png)  
均方误差有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧
氏距离” (Euclidean distance).基于均方误差最小化来进行模型求解的方法称
为“最小二乘法” (least square method).在线性回归中，最小二乘法就是试图
找到一条直线，使所有样本到直线上的欧氏距离之和最小.  
我们可将E(w,b)分别对w和b求导，得到  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232507.png)  
然后令上式为零可得到w和b最优解的闭式(closed-form)解  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232604.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232614.png)  

类似的，可利用最小二乘法来对w和b进行估计.为便于讨论，我们把w
和b吸收入向量形式w = (w; b),相应的，把数据集D表示为一个m x (d + 1)
大小的矩阵X ,其中每行对应于一个示例，该行前d个元素对应于示例的d个
属性值，最后一个元素恒置为1,即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20232924.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20233017.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20233026.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20233034.png)  
当 X^TX 为满秩矩阵或正定矩
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20233151.png)  
线性回归模型为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20233158.png)  
#### 3.3 对数几率回归
是分类任务只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来.
考虑二分类任务，其输出标记。y € {0,1},而线性回归模型产生的预测值
z=w^T* x+b 是实值，于是，我们需将实值z转换为0/1值.  
对数几率函数:  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20234228.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20234244.png)  
对数几率函数是一种"Sigmoid函数”,它将z值转化为一个
接近0或1的g值，并且其输出值在z = 0附近变化很陡.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20234457.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20234506.png)  
若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值 y/(1-y)称为“几率” (odds),反映了亿作为正例的相对可能性.对几率取对数则得到
“对数几率” (log odds,亦称logit) ln(y/(1-y)).  

可通过"极大似然法”来估计w和b.最大化式等价于最小化  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20234958.png)    
以牛顿法为例，其第t + 1轮迭代解的更新公式为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20235009.png)  
#### 3.4 线性判别分析
线性判别分析(Linear Discriminant Analysis,简称LDA)是一种经典的线
性学习方法，LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上,
使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样
本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新
样本的类别  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20235342.png)  
最大化的目标  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20001104.png)   
#### 3.5 多分类学习
现实中常遇到多分类学习任务.有些二分类学习方法可直接推广到多分类,
但在更多情形下，我们是基于一些基本策略，利用二分类学习器来解决多分类
问题.  

不失一般性，考虑N个类别C1,C2,……,Cn,多分类学习的基本思路是
“拆解法”，即将多分类任务拆为若干个二分类任务求解.具体来说，先对问题
进行拆分,然后为拆出的每个二分类任务训练一个分类器;在测试时，对这些分
类器的预测结果进行集成以获得最终的多分类结果.
OvO将这N个类别两两配对，从而产生N(N - 1)/2个二分类任务，例如OvO
将为区分类别Ci和Cj训练一个分类器，该分类器把D中的Ci类样例作为正
例，Cj类样例作为反例.   
OvR则是每次将一个类的样例作为正例、所有其他类的样例作为反例来
训练N个分类器.在测试时若仅有一个分类器预测为正类，则对应的类别标记
作为最终分类结果.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20002507.png)  
MvM是停次将若干个类作为正类，若干个其他类作为反类.  
ECOC ［Dietterich and Bakiri, 1995］是将编码的思想引入类别拆分，并尽
可能在解码过程中具有容错性.ECOC工作过程主要分为两步:

    •编码：对N个类别做肱次划分，每次划分将一部分类别划为正类，一部
    分划为反类，从而形成一个二分类训练集;这样一共产生M个训练集，可
    训练出M个分类器.
    •解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编
    码.将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小
    的类别作为最终预测结果.
类别划分通过“编码矩阵” (coding matrix)指定.编码矩阵有多种形式,
常见的主要有二元码和三元码.前者将每个类别分别指定为正类和反类，后者在正、反类之外，还可指
定“停用类” .
#### 3.6 类别不平衡问题
如果不同类别的训练样例数目稍有差别，通常影响不大，但若差别
很大,则会对学习过程造成困扰•例如有998个反例，但正例只有2个,那么学
习方法只需返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精
度;然而这样的学习器往往没有价值，因为它不能预测出任何正例.  
类别不平衡就是指分类任务中不同类别的训练样例数
目差别很大的情况.
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20003703.png)  
这就是类别不平衡学习的一个基本策略— “再缩放”.
第一类是直接对训练集里的反类样例进行“欠采样”,即去除
一些反例使得正、反例数目接近，然后再进行学习；第二类是对训练集里的
正类样例进行“过采样” ,即增加一些正例使得正、反例数目
接近，然后再进行学习；第三类则是直接基于原始训练集进行学习，但在用
训练好的分类器进行预测时，将上式嵌入到其决策过程中，称为“阈值移
动".
