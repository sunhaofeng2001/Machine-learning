第4章 决策树
=======
#### 4.1 基本流程
决策树(decision tree)是一类常见的机器学习方法.以二分类任务为例，我
们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本
分类的任务，可看作对“当前样本属于正类吗?”这个问题的“决策”或“判
定”过程.顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决
策问题时一种很自然的处理机制.  

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;
叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含
的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集.从
根结点到每个叶结点的路径对应了一个判定测试序列.决策树学习的目的是为
了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简
单且直观的"分而治之”策略.  

显然，决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会
导致递归返回：⑴当前结点包含的样本全属于同一类别，无需划分；(2)当前
属性集为空，或是所有样本在所有属性上取值相同，无法划分；(3)当前结点包
含的样本集合为空，不能划分.  
```
输入：训练集 D = {(x1,x2),(x2,y2)……，(xm,ym)};
     属性集 A = {a1,a2,……,ad}.
过程：函数 TreeGenerate(D , A)
1：生成结点node;
2： if D中样本全属于同一类别C then
3：    将node标记为C类叶结点；return
4： end if
5： if >1 = 0 OR D中样本在A上取值相同then
6:    将node标记为叶结点，其类别标记为D中样本数最多的类;return
7： end if
8： 从A中选择最优划分属性奴； 
9:  for a*的每一个值avs do
10:   为node生成一个分支；令Dv表示D中在如上取值为成的样本子集； 
11:   if Dv 为空 then
12：     将分支结点标记为叶结点，其类别标记为D中样本最多的类;return
13：  else
14：     以 TreeGenerate(Dv,A\{a*}) 为分支结点
15：  end if
16：end for
输出：以node为根结点的一棵决策树
```
#### 4.2 划分选择
由算法4.2可看出，决策树学习的关键是第8行，即如何选择最优划分属
性.一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样
本尽可能属于同一类别，即结点的“纯度”越来越高.
#### 4.2.1 信息增益
“信息嫡” (information entropy)是度量样本集合纯度最常用的一种指标.
假定当前样本集合D中第k类样本所占的比例为饥(& = 1,2,...,|y|), 则D
的信息炳定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20005751.png)  
Ent(D)的值越小，则D的纯度越高.  
假定离散属性Q有U个可能的取值{a1,a2,……,av},若使用a来对样本集
D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在
属性a上取值为a^v的样本，记为D^v.是可计算出用属性a对样本集D进行
划分所获得的"信息增益  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20005944.png)  
一般而言，信息增益越大，则意味着使用属性Q来进行划分所获得的“纯
度提升”越大.因此，我们可用信息增益来进行决策树的划分属性选择.  
#### 4.2.2 增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种
偏好可能带来的不利影响，著名的C4.5决策树算法不直接使
用信息增益，而是使用“增益率”来选择最优划分属性.采用与
相同的符号表示，增益率定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010402.png)  
需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5
算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式
.先从候选划分属性中找出信息增益高于平均水平的属性，再从
中选择增益率最高的. 
#### 4.2.3 基尼指数
CART 决策树使用"基尼指数"(Gini index)来选
择划分属性.数据集D的纯度可用基尼值来度量：  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010626.png)  
直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记
不一致的概率.因此,Gini(D)越小，则数据集Q的纯度越高.
属性q的基尼指数定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010641.png)  
#### 4.3 剪枝处理
剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段.在决策树学
习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决
策树分支过多，这时就可能因训练样本学得“太好” 了，以致于把训练集自身
的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动
去掉一些分支来降低过拟合的风险.  
预剪枝是指在决策树生成过程中，对每个结点在划
分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划
分并将当前结点标记为叶结点.  
后剪枝则是先从训练集生成一棵完整的决策树,
然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.  
 #### 4.3.1 预剪枝
 
