第4章 决策树
=======
#### 4.1 基本流程
决策树(decision tree)是一类常见的机器学习方法.以二分类任务为例，我
们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本
分类的任务，可看作对“当前样本属于正类吗?”这个问题的“决策”或“判
定”过程.顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决
策问题时一种很自然的处理机制.  

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点;
叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含
的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集.从
根结点到每个叶结点的路径对应了一个判定测试序列.决策树学习的目的是为
了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简
单且直观的"分而治之”策略.  

显然，决策树的生成是一个递归过程.在决策树基本算法中，有三种情形会
导致递归返回：⑴当前结点包含的样本全属于同一类别，无需划分；(2)当前
属性集为空，或是所有样本在所有属性上取值相同，无法划分；(3)当前结点包
含的样本集合为空，不能划分.  
```
输入：训练集 D = {(x1,x2),(x2,y2)……，(xm,ym)};
     属性集 A = {a1,a2,……,ad}.
过程：函数 TreeGenerate(D , A)
1：生成结点node;
2： if D中样本全属于同一类别C then
3：    将node标记为C类叶结点；return
4： end if
5： if >1 = 0 OR D中样本在A上取值相同then
6:    将node标记为叶结点，其类别标记为D中样本数最多的类;return
7： end if
8： 从A中选择最优划分属性奴； 
9:  for a*的每一个值avs do
10:   为node生成一个分支；令Dv表示D中在如上取值为成的样本子集； 
11:   if Dv 为空 then
12：     将分支结点标记为叶结点，其类别标记为D中样本最多的类;return
13：  else
14：     以 TreeGenerate(Dv,A\{a*}) 为分支结点
15：  end if
16：end for
输出：以node为根结点的一棵决策树
```
#### 4.2 划分选择
由算法4.2可看出，决策树学习的关键是第8行，即如何选择最优划分属
性.一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样
本尽可能属于同一类别，即结点的“纯度”越来越高.
#### 4.2.1 信息增益
“信息嫡” (information entropy)是度量样本集合纯度最常用的一种指标.
假定当前样本集合D中第k类样本所占的比例为饥(& = 1,2,...,|y|), 则D
的信息炳定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20005751.png)  
Ent(D)的值越小，则D的纯度越高.  
假定离散属性Q有U个可能的取值{a1,a2,……,av},若使用a来对样本集
D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在
属性a上取值为a^v的样本，记为D^v.是可计算出用属性a对样本集D进行
划分所获得的"信息增益  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20005944.png)  
一般而言，信息增益越大，则意味着使用属性Q来进行划分所获得的“纯
度提升”越大.因此，我们可用信息增益来进行决策树的划分属性选择.  
#### 4.2.2 增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种
偏好可能带来的不利影响，著名的C4.5决策树算法不直接使
用信息增益，而是使用“增益率”来选择最优划分属性.采用与
相同的符号表示，增益率定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010402.png)  
需注意的是，增益率准则对可取值数目较少的属性有所偏好，因此，C4.5
算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式
.先从候选划分属性中找出信息增益高于平均水平的属性，再从
中选择增益率最高的. 
#### 4.2.3 基尼指数
CART 决策树使用"基尼指数"(Gini index)来选
择划分属性.数据集D的纯度可用基尼值来度量：  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010626.png)  
直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记
不一致的概率.因此,Gini(D)越小，则数据集Q的纯度越高.
属性q的基尼指数定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20010641.png)  
#### 4.3 剪枝处理
剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段.在决策树学
习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决
策树分支过多，这时就可能因训练样本学得“太好” 了，以致于把训练集自身
的一些特点当作所有数据都具有的一般性质而导致过拟合.因此，可通过主动
去掉一些分支来降低过拟合的风险.  
预剪枝是指在决策树生成过程中，对每个结点在划
分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划
分并将当前结点标记为叶结点.  
后剪枝则是先从训练集生成一棵完整的决策树,
然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.  
#### 4.3.1 预剪枝
基于信息增益准则，我们会选取属性“脐部”来对训
练集进行划分，并产生3个分支.然而，是否应该进行这个划分
呢？预剪枝要对划分前后的泛化性能进行估计.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20175643.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20175655.png)  
预剪枝使得决策树的很多分支都没有“展
开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测
试时间开销.但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可
能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显
著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了
欠拟合的风险.  
#### 4.3.2 后剪枝
后剪枝先从训练集生成一棵完整决策树，
后剪枝首先考察图4.5中的结点6.若将其领衔的分支剪除，则相当于
把6替换为叶结点.替换后的叶结点包含编号为｛7,15｝的训练样本，于是，该
叶结点的类别标记为"好瓜"，此时决策树的验证集精度提高至57.1%.于是,
后剪枝策略决定剪枝，如图4.7所示.
然后考察结点5，若将其领衔的子树替换为叶结点，则替换后的叶结点包
含编号为｛6,7,15｝的训练样例，叶结点类别标记为“好瓜”,此时决策树验证
集精度仍为57.1%.于是，可以不进行剪枝.
对结点2，若将其领衔的子树替换为叶结点，则替换后的叶结点包含编号
为｛1,2,3,14｝的训练样例，叶结点标记为“好瓜”.此时决策树的验证集精度
提高至71.4%.于是，后剪枝策略决定剪枝.
对结点3和1，若将其领衔的子树替换为叶结点，则所得决策树的验证集
精度分别为71.4%与42.9%,均未得到提高.于是它们被保留.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20180040.png)  
后剪枝决策树通常比预剪枝决策树保留了更
多的分支.一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预
剪枝决策树.但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上
地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树
和预剪枝决策树都要大得多.
#### 4.4 连续与缺失值
#### 4.4.1 连续值处理
由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可
取值来对结点进行划分.此时，连续属性离散化技术可派上用场.最简单的策
略是采用二分法(bi-partition)对连续属性进行处理，这正是C4.5决策树算法中
采用的机制.  
给定样本集D和连续属性a,假定a在上出现了n个不同的取值，将这
些值从小到大进行排序，记为{a1,a2,a3,……,an}. 基于划分点t可将D分为子集
Dt-和Dt+其中Dt-包含那些在属性a上取值不大于t的样本，而Dt+则包含
那些在属性a上取值大于t的样本.显然，对相邻的属性取值ai与ai-1来说,t
在区间［ai,ai+1)中取任意值所产生的划分结果相同.因此,对连续属性a,我们
可考察包含n-1个元素的候选划分点集合  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20191532.png)  
即把区间［ai,ai+1)的中位点(a^i+a^i+1)/2作为候选划分点.然后，我们就可像离散
属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分.例如,
可对式(4.2)稍加改造：  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20191557.png)  
其中Gain(D,a,t)是样本集D基于划分点t二分后的信息增益.于是，我们就
可选择使Gain(D,a,t)最大化的划分点.  
需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还
可作为其后代结点的划分属性.
#### 4.4.2 缺失值处理
现实任务中常会遇到不完整样本，即样本的某些属性值缺失.
#### 4.5 多变量决策树
若我们把每个属性视为坐标空间中的一个坐标轴，则衫个属性描述的样本
就对应了 d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻
找不同类样本之间的分类边界.决策树所形成的分类边界有一个明显的特点:
轴平行（axis-parallel）,即它的分类边界由若干个与坐标轴平行的分段组成.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20192618.png)  
以表4.5中的西瓜数据3.0a为例，将它作为训练集可学得图4.10所示的决
策树，这棵树所对应的分类边界如图4.11所示.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20192757.png)  
显然，分类边界的每一段都是与坐标轴平行的.这样的分类边界使得学习
结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值.但在学
习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似,  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20192820.png)  
如图4.12所示；此时的决策树会相当复杂，由于要进行大量的属性测试，预测
时间开销会很大.
