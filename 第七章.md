第8章 集成学习
===========
#### 8.1 个体与集成
集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任
务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学
习(committee-based learning)等.   
先产生一组“个体学习
器”(individual learner),再用某种策略将它们结合起来.个体学习器通常
由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网
络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”
中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”
的(homogeneous).同质集成中的个体学习器亦称"基学习器” (base learner),
相应的学习算法称为“基学习算法”(base learning algorithm).集成也可包含
不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异
质” (heterogenous).异质集成中的个体学习器由不同的学习算法生成，这时
就不再有基学习算法;相应的，个体学习器一般不称为基学习器，常称为“组件
学习器” (component learner)或直接称为个体学习器.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20112820.png)  
集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的
泛化性能.这对“弱学习器”(weak learner)尤为明显，因此集成学习的很多理
论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器.但
需注意的是，虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习
器的一些经验等，人们往往会使用比较强的学习器.  
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器
间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting,后
者的代表是Bagging和"随机森林"(Random Forest).  
#### 8.2 Boosting 
Boosting是一族可将弱学习器提升为强学习器的算法.这族算法的工作机
制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练
样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注,
然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学
习器数目达到事先指定的值T,最终将这T个基学习器进行加权结合.  
AdaBoost算法有多种推导方式，比较容易理解的是基于“加性模
型”(additive model),即基学习器的线性组合  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20113238.png)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20113253.png)  
在AdaBoost算法中，第一个基分类器h1是通过直接将基学习算法用于初
始数据分布而得;此后迭代地生成ht和at,当基分类器ht基于分布Dt产生后,
该基分类器的权重at应使得atht最小化指数损失函数  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20113523.png)  
AdaBoost算法在获得Ht-1之后样本分布将进行调整，使下一轮的基学习
器ht能纠正Hji的一些错误.理想的如能纠正Rt的全部错误，即最小化  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20113548.png)  
#### 8.3 Bagging与随机森林 
给定一个训练数据集，一种可能的做法是对训练样本进行采
样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器.这
样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异.然而，为
获得好的集成，我们同时还希望个体学习器不能太差.如果采样出的每个子集
都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有
效学习，这显然无法确保产生出比较好的基学习器.为解决这个问题，我们可考
虑使用相互有交叠的采样子集.
#### 8.3.1 Baggin
Bagging是并行式集成学习方法最著名的代表.给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该
样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m
次随机采样操作，我们得到含m个样本的采样集,初始训练集中有的样本在采
样集里多次出现，有的则从未出现.  
照这样，我们可采样出:T个含m个训练样本的采样集，然后基于每个采样
集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging的基本
流程.在对预测输出进行结合时,Bagging通常对分类任务使用简单投票法，对
回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形，则
最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最
终胜者.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20114659.png)
#### 8.3.2 随机森林
随机森林(Random Forest,简称 RF)是 Bagging 的一个
扩展变体.RF在以决策树为基学习器构建Bagging集成的基础上，进一步在
决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分
属性时是在当前结点的属性集合(假定有d个属性)中选择一个最优属性；而在
RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分.这里的参数
k控制了随机性的引入程度：若令k = d,则基决策树的构建与传统决策树相同;
若令k = 1,则是随机选择一个属性用于划分；一-般情况下，推荐值k = log2 d.
#### 8.4 结合策略
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20123713.png) 
#### 8.4.1 平均法
对数值型输出hi(x) e R,最常见的结合策略是使用平均法(averaging).
•简单平均法(simple averaging)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20123932.png)  
•加权平均法(weighted averaging)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20123957.png)  
加权平均法的权重一般是从训练数据中学习而得，现实任务中的训练样本
通常不充分或存在噪声，这将使得学出的权重不完全可靠.尤其是对规模比较
大的集成来说，要学习的权重比较多，较容易导致过拟合.因此，实验和应用均
显示出，加权平均法未必一定优于简单平均法. 一般而言，在个体学习器性能相差较大时宜使用加权平均
法,而在个体学习器性能相近时宜使用简单平均法.
#### 8.4.2 投票法
对分类任务来说,学习器衍将从类别标记集合｛勺,％...，取｝中预测出一
个标记，最常见的结合策略是使用投票法(voting)为便于讨论，我们将用在样
本x上的预测输出表示为一个N维向量（h1(x);h2(x);……;hN(x)）；就其中hji(x)
是hi在类别标记Cj上的输出.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20124404.png)
#### 8.4.3 学习法
当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过
另一个学习器来进行结合stacking是学习法
的典型代表.这里我们把个体学习器称为初级学习器，用于结合的学习器称为
次级学习器或元学习器(meta-learner).  
Stacking先从初始数据集训练出初级学习器，然后“生成” 一个新数据集
用于训练次级学习器.在这个新数据集中，初级学习器的输出被当作样例输入
特征，而初始样本的标记仍被当作样例标记.Stacking的算法描述如下图所
示，这里我们假定初级学习器使用不同学习算法产生，即初级集成是异质的.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20124614.png)  
#### 8.5 多样性
#### 8.5.1 误差-分歧分解
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20125249.png)  
#### 8.5.2 多样性度量
多样性度量(diversity measwe)是用于度量集成中个体分类器的
多样性，即估算个体学习器的多样化程度.典型做法是考虑个体分类器的两两
相似/不相似性.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20125457.png)
#### 8.5.3 多样性增强
在学习过程中引
入随机性，常见做法主要是对数据样本、输入属性、输出表示、算法参数进行
扰动以增强多样性.  
•数据样本扰动  
数据样本扰动通常是基于采样法，例如在Bagging
中使用自助采样，在AdaBoost中使用序列采样.  
•输入属性扰动  
训练样本通常由一组属性描述，不同的“子空间"提供了观察数据的不同视角.显然，从不同子空间训练出的个体学习器必然
有所不同.著名的随机子空间算法就依赖于输入
属性扰动，该算法从初始属性集中抽取出若干个属性子集，再基于每个属性子
集训练一个基学习器.    
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-25%20125854.png)  
•输出表示扰动  
此类做法的基本思路是对输出表示进行操纵以增强多样性.可对训练样本
的类标记稍作变动，如"翻转法”随机改变
一些训练样本的标记；也可对输出表示进行转化，如“输出调制法” 将分类输出转化为回归输出后构建个体学习器;
还可将原任务拆解为多个可同时求解的子任务.  
•算法参数扰动  
基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初
始连接权值等，通过随机设置不同的参数，往往可产生差别较大的个体学习器.  
如“负相关法"显式地通过正则
化项来强制个体神经网络使用不同的参数.对参数较少的算法，可通过将其学
习过程中某些环节用其他类似方式代替，从而达到扰动的目的.
