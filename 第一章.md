第2章 模型评估与选择
=======
#### 2.1 经验误差与过拟合
通常我们把分类错误的样本数占样本总数的比例称为“错误率” (error
rate),即如果在m个样本中有a个样本分类错误，则错误率E = a/m;相应的,
1-a/m称为“精度” (accuracy),即“精度=1—错误率”.更一般地,我们把
学习器的实际预测输出与样本的真实输出之间的差异称为“误差” (error),
学习器在训练集上的误差称为“训练误差” (training error)或“经验误
差”(empirical error),在新样本上的误差称为“泛化误差”(generalization
error).  

当学习器把训练样本学得“太
好” 了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都
会具有的一般性质，这样就会导致泛化性能下降.这种现象在机器学习中称为
“过拟合”(overfitting).与“过拟合”相对的是"欠拟合”(underEtting),这
是指对训练样本的一般性质尚未学好.  
#### 2.2 评估方法
通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选
择.为此，需使用一个“测试集"(testing set)来测试学习器对新样本的判别能
力，然后以测试集上的“测试误差” (testing error)作为泛化误差的近似.通常
我们假设测试样本也是从样本真实分布中独立同分布采样而得.但需注意的
是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未
在训练过程中使用过.  

通过对数据集D进行适当的处理，从中产生出训练集S和测试集T.  

#### 2.2.1 留出法
“留出法”(hold-out)直接将数据集刀划分为两个互斥的集合，其中一个
集合作为训练集S,另一个作为测试集厂即 D = SUT, SOT = 0.在S上训
练出模型后，用:r来评估其测试误差，作为对泛化误差的估计.  

需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免
因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中
至少要保持样本的类别比例相似.如果从采样(sampling)的角度来看待数据
集的划分过程，则保留类别比例的采样方式通常称为“分层采样” (stratified
sampling）.    
#### 2.2.2 交叉验证法
“交叉验证法”(cross validation)先将数据集D划分为k个大小相似的
互斥子集，即D = D1 U D2 U …… U Di U Dk, Di()Dj=0.  
每个子集Di都
尽可能保持数据分布的一致性，即从D中通过分层采样得到.然后，每次用
k - 1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k
组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果
的均值.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20205231.png)  
与留出法相似，将数据集D划分为k个子集同样存在多种划分方式.为
减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分
重复p次，最终的评估结果是这p次化折交叉验证结果的均值.
#### 2.2.3 自助法
“自助法”(bootstrapping)是一个比较好的解决方案，它直接以自助采样
法(bootstrap sampling)为基础.给定包含 m 个样
本的数据集D ,我们对它进行采样产生数据集每次随机从D中挑选一个
样本,将其拷贝放入D',然后再将该样本放回初始数据集D中,使得该样本在
下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m
个样本的数据集 D', 这就是自助采样的结果.  

自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法
能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处.
然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.因此，在初始数据量足够时，留出法和交叉验证法更常用一些.
#### 2.2.4 调参与最终模型
大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模
型的性能往往有显著差别.因此,在进行模型评估与选择时，除了要对适用学习
算法进行选择，还需对算法参数进行设定，这就是通常所说的“参数调节”或
简称"调参” (parameter tuning).  

学习算法的很多参数是在实数范围内取值，因此，对每种参数
配置都训练出模型来是不可行的.现实中常用的做法，是对每个参数选定一个
范围和变化步长,例如在［0,0.2］范围内以0.05为步长,则实际要评估的候选参
数值有5个，最终是从这5个候选值中产生选定值.  

给定包含m个样本的数据集7?,在模型评估与选择过程中由于需要留出
一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型.因此，在
模型选择完成后，学习算法和参数配置已选定，此时应该用数据集Q重新训练
模型.这个模型在训练过程中使用了所有m个样本，这才是我们最终提交给用
户的模型.  

我们通常把学得模型在实际使用中遇到的数据称为测
试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为“验
证集”(validation set).  

#### 2.3 性能度量
对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需
要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure).  

回归任务最常用的性能度量是“均方误差"(mean squared error)  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20211456.png)  
更一般的，对于数据分布T>和概率密度函数p(-),均方误差可描述为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20211505.png)  
#### 2.3.1 错误率与精度
对样例集D,分类错误率定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20212150.png)  
精度则定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20212207.png)  
#### 2.3.2 查准率、查全率与F1
对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划
分为真正例(true positive)、假正例(false positive)> 真反例(true negative)>
假反例(false negative)H种情形，令TP、FP、TN、EN分别表示其对应的
样例数，则显然有TP + FP + TN + FN = 样例总数.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20215902.png)    
查准率P与查全率R分别定义为  
P = TP/(TP+FP)  
R = TP/(TP+FN)  
查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往
偏低；而查全率高时，查准率往往偏低.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20220540.png)  
#### 2.3.3 ROC 与 AUC
很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与
一个分类阈值(threshold)进行比较，若大于阈值则分为正类，否则为反类.  

在不同的应用任务中，我们可根据任务需求来采用不同的截断点，例如若
我们更重视“查准率”，则可选择排序中靠前的位置进行截断；若更重视“查
全率”，则可选择靠后的位置进行截断.因此，排序本身的质量好坏，体现了综
合考虑学习器在不同任务下的“期望泛化性能”的好坏.  

是比较ROC曲线下的面积，即AUC.  
从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20221110.png)  
形式化地看,AUC考虑的是样本预测的排序质量，因此它与排序误差有紧
密联系.给定m+个正例和m-个反例，令D+和D-分别表示正、反例集合,
则排序“损失”(loss)定义为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20221159.png)  
#### 2.3.4 代价敏感错误率与代价曲线
为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”.  

以二分类任务为例，我们可根据任务的领域知识设定一个“代价矩
阵” (cost matrix)，costij表示将第i类样本预测为第j类
样本的代价.一般来说，costij = 0;若将第0类判别为第1类所造成的损失更
大，则cost01 > cost10；损失程度相差越大，cost01与cost10 值的差别越大.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20221857.png)  
第0类作为正
类、第1类作为反类，令D+与D-分别代表样例集D的正例子集和反例子
集则"代价敏感”(cost-sensitive)错误率为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20222009.png)  
代价曲线图的横轴是取值为［0,1］的正例概率代价  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20222128.png)  
其中p是样例为正例的概率；纵轴是取值为[0,1]的归一化代价  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20222139.png)  
#### 2.4 4比较检验
先使用某种实验评估方法测得学习器的某个性能度量结果，然后对这些结
果进行比较.
#### 2.4.1 假设检验
假设检验中的“假设”是对学习器泛化错误率分布的某种判断或猜想,现实任务中我们并不知道学习器的泛化错误率，只能获知其测试错
误率.  
泛化错误率为€的学习器在一个样本上犯错的概率是€；测试错误率€意味
着在m个测试样本中恰有€ x m个被误分类.假定测试样本是从样本总体分布
中独立采样而得，那么泛化错误率为€的学习器将其中mf个样本误分类、其
余样本全都分类正确的概率是 €m'(l - €)m-m'.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20223630.png)
#### 2.4.2 交叉验证t检验
欲进行有效的假设检验，一个重要前提是测试错误率均为泛化错误率的独
立采样然而，通常情况下由于样本有限，在使用交叉验证等实验估计方法时,
不同轮次的训练集会有一定程度的重叠，这就使得测试错误率实际上并不独立,
会导致过高估计假设成立的概率.为缓解这一问题,可采用“5 x 2交叉验证法.
5 x 2交叉验证是做5次2折交叉验证,在每次2折交叉验证之前随机将数
据打乱，使得5次交叉验证中的数据划分不重复.
#### 2.4.3 McNemar 检验
对二分类问题，使用留出法不仅可估计出学习器A和B的测试错误率，还
可获得两学习器分类结果的差别，即两者都正确、都错误、一个正确另一个错
误的样本数.  
![imgae](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20224958.png)  
若我们做的假设是两学习器性能相同，则应有e01 = e10,那么变量
|e01- e10|应当服从正态分布，且均值为1，方差为e01+ e10.因此变量  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225007.png)  
服从自由度为1的X^2分布，即标准正态分布变量的平方.
#### 2.4.4 Friedman检验与Nemenyi后续检验
交叉验证t检验和McNemar检验都是在一个数据集上比较两个算法的
性能，而在很多时候，我们会在一组数据集上对多不算法进行比较.当有多个
算法参与比较时，一种做法是在每个数据集上分别列出两两比较的结果，而在
两两比较时可使用前述方法；另一种方法更为直接，即使用基于算法排序的
Friedman 检验.
#### 2.5 偏差与方差
“偏差-方差分解"(bias-variance decomposition)是解
释学习算法泛化性能的一种重要工具.  
以回归任务为例，学习算法的期望预测为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225440.png)    
使用样本数相同的不同训练集产生的方差为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225451.png)  
噪声为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225506.png)  
期望输出与真实标记的差别称为偏差(bias),即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225519.png)  
一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境.  
给定学习任务，假定我们能控制学习算法
的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足
以使学习器产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深， 学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差'
逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练
数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全
局的特性被学习器学到了，则将发生过拟合.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-23%20225839.png)  
