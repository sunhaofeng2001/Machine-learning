第6章 支持向量机
========
#### 6.1 间隔与支持向量
给定训练样本集 D= {(x1,y1),(x2,y2),……,(xm,ym)},yi={-1,+1}.分
类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将
不同类别的样本分开.  
直观上看，应该去找位于两类训练样本“正中间”的划分超平面,但是由于训练集的局限性或噪声的因素，训练集外的样本可能比图6.1中的训练
样本更接近两个类的分隔界，这将使许多划分超平面出现错误，而红色的超平
面受影响最小.换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见
示例的泛化能力最强.  
在样本空间中，划分超平面可通过如下线性方程来描述:  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20215300.png)  
其中M = (w1；w2;...;wd)为法向量，决定了超平面的方向；b为位移项，决定
了超平面与原点之间的距离.显然，划分超平面可被法向量w和位移b确定,下面我们将其记为(w,b).样本空间中任意点。到超平面(w,b)的距离可写为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20215416.png)  
假设超平面(w.b)能将训练样本正确分类，即对于(xi,yi)£ D,若yi=
+1,则有 wTxi + b〉0;若yi=一1,则有 wTxi + b <0.令  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20215605.png)  
距离超平面最近的这几个训练样本点使式(6.3)的等号成立,
它们被称为“支持向量” (support vector),两个异类支持向量到超平面的距离
之和为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20220057.png)  
它被称为“间隔” (margin).  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20220131.png)  
欲找到具有“最大间隔”(maximum margin)的划分超平面，也就是要找
到能满足式(6.3)中约束的参数w和饥使得7最大，即  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20220158.png)  
显然，为了最大化间隔，仅需最大化iiwr\这等价于最小化iiwii2.于是,
式(6.5)可重写为  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20220215.png)  
这就是支持向量机(Support Vector Machine,简称SVM)的基本型.
#### 6.2 对偶问题
我们希望求解式(6.6)来得到大间隔划分超平面所对应的模型  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20220215.png)  
其中s和b是模型参数.   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20222511.png)    
SMO的基本思路是先固定ai之外的所有参数，然后求ai上的极值.由于
存在约束E ai* yi=0,若固定之外的其他变量，则叫可由其他变量导出.
于是，SMO每次选择两个变量ai和aj,并固定其他参数.这样，在参数初始化
后，SMO不断执行如下两个步骤直至收敛:  
选取一对需更新的变量ai和aj;  
固定ai和aj以外的参数，求解下式获得更新后的ai和aj.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20223741.png)  
#### 6.3 核函数
训练样本是线性可分的，即存在一个划分
超平面能将训练样本正确分类.然而在现实任务中，原始样本空间内也许并不
存在一个能正确划分两类样本的超平面.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20224449.png)  
对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得
样本在这个特征空间内线性可分.例如在图6.3中，若将原始的二维空间映射
到一个合适的三维空间，就能找到一个合适的划分超平面.幸运的是，如果原始
空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分.  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20224734.png)    
#### 6.4 软间隔与正则化
，在现实任务
中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；退一步说,
即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个
貌似线性可分的结果不是由于过拟合所造成的.
缓解该问题的一个办法是允许支持向量机在一些样本上出错.为此，要引
入"软间隔"(soft margin)的概念，如图6.4所示.   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20225955.png)   
具体来说，前面介绍的支持向量机形式是要求所有样本均满足约束(6.3),
即所有样本都必须划分正确，这称为“硬间隔”(hard margin),而软间隔则是
允许某些样本不满足约束  
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20230250.png)  
#### 6.5 支持向量回归
现在我们来考虑回归问题.给定训练样本 D = {(x1,y1)(x2,y2),……,(xm,ym)},yi€R.
希望学得一个形如式(6.7)的回归模型，使得f(x)与y尽可
能接近,w和b是待确定的模型参数.
对样本(w,y),传统回归模型通常直接基于模型输出f(x)与真实输出y之
间的差别来计算损失，当且仅当与y完全相同时,损失才为零.与此不同,
支持向量回归假设我们能容忍f(x)与y之间最多有€的偏差，即仅当f(x)与y之间的差别绝对值大于e时才计算损
失.如图6.6所示，这相当于以为中心，构建了一个宽度为2e的间隔带，若
训练样本落入此间隔带，则认为是被预测正确的.   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20232302.png)   
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20232920.png)  
#### 6.6 核方法
![image](https://github.com/sunhaofeng2001/Machine-learning/blob/master/IMG/%E6%89%B9%E6%B3%A8%202020-08-24%20233515.png)
